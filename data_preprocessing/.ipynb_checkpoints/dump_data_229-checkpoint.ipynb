{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import librosa as lr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "from IPython.display import display, Audio\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../util/tokenizer.txt','r') as infile:\n",
    "    data = json.load(infile)\n",
    "    tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "VOCAB_SIZE = len(tokenizer.word_index)\n",
    "TEXT_PADDING = 8\n",
    "SAMPLING_RATE = 8000\n",
    "WINDOW_SIZE = 1000 #in ms\n",
    "SLIDE_STRIDE = 300 # in ms\n",
    "NUM_COEFFS = 16\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript_paths(audio_paths):\n",
    "    transcript_paths = [] \n",
    "    for audio_path in audio_paths:\n",
    "        rest = audio_path.strip('../data/')\n",
    "        i = rest.find('/')\n",
    "        folder_num = rest[:i]\n",
    "        start_at = len('../data/' + folder_num +'/audio/')\n",
    "        index = audio_path.find('/',start_at)\n",
    "        id = audio_path[start_at:index]\n",
    "        transcript_paths.append('../data/'+folder_num+'/conversations/'+id+'.json')\n",
    "    return transcript_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = glob.glob('../data/*/audio/*/*.wav')\n",
    "print(audio_paths[0])\n",
    "print(len(audio_paths))\n",
    "transcript_paths = get_transcript_paths(audio_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio(clip,sr):\n",
    "    display(Audio(clip,rate=sr))\n",
    "    \n",
    "def audio_to_mfcc(samples, n_mfcc=NUM_COEFFS, hop_length=512,n_fft=2048):\n",
    "    mfcc = lr.feature.mfcc(samples, sr = SAMPLING_RATE, n_mfcc=n_mfcc, hop_length=hop_length,n_fft=n_fft).T\n",
    "    return mfcc\n",
    "\n",
    "def compute_windows(segment,agent_start_ms):\n",
    "    num_words = len(segment['word_offsets_ms'])\n",
    "    if(num_words == 0):\n",
    "        return []\n",
    "    tokens = segment['transcript'].split()\n",
    "    windows = []\n",
    "    window_offset_ms = segment['start_ms'] + WINDOW_SIZE\n",
    "    #compute the transcript of the window\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    while(start_index < num_words and segment['start_ms'] + segment['word_offsets_ms'][start_index] + segment['word_durations_ms'][start_index]< window_offset_ms-WINDOW_SIZE):\n",
    "        start_index += 1\n",
    "    while(end_index < num_words and segment['start_ms'] + segment['word_offsets_ms'][end_index] < window_offset_ms):\n",
    "        end_index += 1\n",
    "    transcript = ' '.join(tokens[start_index:end_index])\n",
    "    encoded_transcript = tokenizer.texts_to_sequences([transcript])\n",
    "    padded_transcript = tf.keras.preprocessing.sequence.pad_sequences(encoded_transcript, maxlen=TEXT_PADDING)\n",
    "    end_ms = agent_start_ms\n",
    "    windows.append((window_offset_ms,padded_transcript[0]))\n",
    "    while(window_offset_ms < end_ms):\n",
    "        window_offset_ms = min(end_ms, window_offset_ms + SLIDE_STRIDE)\n",
    "        while(start_index < num_words and segment['start_ms'] + segment['word_offsets_ms'][start_index] + segment['word_durations_ms'][start_index] < window_offset_ms-WINDOW_SIZE):\n",
    "            start_index += 1\n",
    "        while(end_index < num_words and segment['start_ms'] + segment['word_offsets_ms'][end_index] < window_offset_ms):\n",
    "            end_index += 1\n",
    "        transcript = ' '.join(tokens[start_index:end_index])\n",
    "        encoded_transcript = tokenizer.texts_to_sequences([transcript])\n",
    "        padded_transcript = tf.keras.preprocessing.sequence.pad_sequences(encoded_transcript, maxlen=TEXT_PADDING)\n",
    "        windows.append((window_offset_ms,padded_transcript[0]))\n",
    "    return windows\n",
    "\n",
    "def get_channel(audio, segments):\n",
    "    if(len(segments) > 0):\n",
    "        start_ms = segments[0][0]['start_ms']\n",
    "        end_ms = segments[0][0]['start_ms'] + segments[0][0]['duration_ms']\n",
    "        start_sample = lr.core.time_to_samples(start_ms/1000, sr=SAMPLING_RATE)\n",
    "        end_sample = lr.core.time_to_samples(end_ms/1000, sr=SAMPLING_RATE)\n",
    "        norms = np.zeros(2)\n",
    "        norms[0] = np.linalg.norm(audio[0][start_sample:end_sample])\n",
    "        norms[1] = np.linalg.norm(audio[1][start_sample:end_sample])\n",
    "        return np.argmax(norms)\n",
    "    return -1\n",
    "    \n",
    "def get_clip(audio,window_end_ms):\n",
    "    start_ms = window_end_ms - WINDOW_SIZE\n",
    "    start_sample = lr.core.time_to_samples(start_ms/1000, sr=SAMPLING_RATE)\n",
    "    end_sample = lr.core.time_to_samples(window_end_ms/1000, sr=SAMPLING_RATE)        \n",
    "    return audio[start_sample:end_sample]\n",
    "\n",
    "def get_training_segments(segments):\n",
    "    positive = []\n",
    "    negative = []\n",
    "    i = 0\n",
    "    num_segments = len(segments)\n",
    "    while(i < num_segments):\n",
    "        if (segments[i]['duration_ms']<WINDOW_SIZE or segments[i]['caller_role'] == 'AG' or i == num_segments-1):\n",
    "            i+=1\n",
    "            continue\n",
    "        elif (segments[i+1]['caller_role'] == 'CL'):\n",
    "            negative.append((segments[i],segments[i+1],i))\n",
    "            i+=1\n",
    "        else:\n",
    "            silent_time_ms = segments[i+1]['start_ms'] - (segments[i]['start_ms']+segments[i]['duration_ms'])\n",
    "            if(silent_time_ms > 0):\n",
    "                positive.append((segments[i],segments[i+1],i))\n",
    "            i+=2\n",
    "    return positive, negative\n",
    "\n",
    "def get_label(window_end_ms, agent_start_ms):\n",
    "    return int(window_end_ms >= agent_start_ms - 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    count = 0\n",
    "    for audio_path, transcript_path in zip(audio_paths, transcript_paths):\n",
    "        count += 1\n",
    "        print(\"Processed \", count , \" files\")\n",
    "        try:\n",
    "            with open(transcript_path,'r') as conv_json, open(audio_path,'rb') as conv_audio:\n",
    "                audio, sr = lr.load(conv_audio,sr=SAMPLING_RATE, mono=False)\n",
    "                transcript = json.loads(conv_json.read())\n",
    "            pos_segments, neg_segments = get_training_segments(sorted(transcript['segments'],key = lambda x:x['start_ms']))\n",
    "            caller_channel = get_channel(audio,pos_segments) if len(pos_segments) > 0 else get_channel(audio, neg_segments)\n",
    "\n",
    "            for i in range(len(pos_segments)):\n",
    "                start_ms = pos_segments[i][0]['start_ms']\n",
    "                windows = compute_windows(pos_segments[i][0],pos_segments[i][1]['start_ms'])\n",
    "                for window_end_ms,encoded_transcript in windows:\n",
    "                    clip = get_clip(audio[caller_channel],window_end_ms)\n",
    "                    mfcc = audio_to_mfcc(clip)\n",
    "                    label = get_label(window_end_ms,pos_segments[i][1]['start_ms'])\n",
    "                    clip_address = audio_path + '_' + str(pos_segments[i][2]) + '_' + str(window_end_ms-start_ms)\n",
    "                    data.append([mfcc,encoded_transcript,label,clip_address])\n",
    "\n",
    "            for i in range(len(neg_segments)):\n",
    "                start_ms = neg_segments[i][0]['start_ms']\n",
    "                windows = compute_windows(neg_segments[i][0],neg_segments[i][1]['start_ms'])\n",
    "                for window_end_ms, encoded_transcript in windows:\n",
    "                    clip = get_clip(audio[caller_channel],window_end_ms)\n",
    "                    mfcc = audio_to_mfcc(clip)\n",
    "                    label = 0\n",
    "                    clip_address = audio_path + '_' + str(neg_segments[i][2]) + '_' + str(window_end_ms-start_ms)\n",
    "                    data.append([mfcc,encoded_transcript,label,clip_address])\n",
    "        except:\n",
    "            print(\"problem loading a file\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for w,t,s in [(4000,32,63)]:\n",
    "    data = []\n",
    "    WINDOW_SIZE = w\n",
    "    TEXT_PADDING = t\n",
    "    create_data()\n",
    "    \n",
    "    data = list(filter(lambda x: x[0].shape==(s,NUM_COEFFS) ,data))\n",
    "    pos_data = list(filter(lambda x: x[2] == 1,data))\n",
    "    neg_data = list(filter(lambda x: x[2] == 0, data))\n",
    "    neg_data = random.sample(neg_data,2*len(pos_data))\n",
    "    data_updated = pos_data + neg_data\n",
    "    random.shuffle(data_updated)\n",
    "    \n",
    "    X_mfccs = []\n",
    "    X_transcripts = []\n",
    "    Y = []\n",
    "    Clips = []\n",
    "    for x_mfcc,x_transcript, y, clip_location in data_updated:\n",
    "        X_mfccs.append(x_mfcc)\n",
    "        X_transcripts.append(x_transcript)\n",
    "        Y.append(y)\n",
    "        Clips.append(clip_location)\n",
    "        \n",
    "    X_mfccs = np.array(X_mfccs)\n",
    "    print(X_mfccs.shape)\n",
    "    X_transcripts = np.array(X_transcripts)\n",
    "    print(X_transcripts.shape)\n",
    "    Y = np.array(Y)\n",
    "    print(Y.shape)\n",
    "    Clips = np.array(Clips)\n",
    "    \n",
    "    \n",
    "    np.save('../util/X_mfccs_error_4000.npy',X_mfccs)\n",
    "    np.save('../util/X_clips_error_4000.npy',Clips)\n",
    "    np.save('../util/X_transcripts_error_4000.npy',X_transcripts)\n",
    "    np.save('../util/labels_error_4000.npy',Y)\n",
    "    #Save the Dataset\n",
    "#     mfcc_name = '../util/X_mfccs_' + str(w) + '.npy'\n",
    "#     clips_name = '../util/clips_' + str(w) + '.npy'\n",
    "#     transcript_name = '../util/X_transcripts_' + str(w) + '.npy'\n",
    "#     label_name = '../util/labels_' + str(w) + '.npy'\n",
    "#     np.save(mfcc_name,X_mfccs)\n",
    "#     np.save(clips_name,Clips)\n",
    "#     np.save(transcript_name,X_transcripts)\n",
    "#     np.save(label_name,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAIN DEV TEST SPLITS\n",
    "# X_train_mfcc, X_test_mfcc, X_train_transcript, X_test_transcript, y_train, y_test = train_test_split(X_mfccs,X_transcripts,Y,test_size=0.2)\n",
    "# print(X_train_mfcc.shape)\n",
    "# print(X_train_transcript.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZE\n",
    "# mu = np.mean(X_train_mfcc,axis=0)\n",
    "# std_dev = np.std(X_train_mfcc,axis=0)\n",
    "# np.save('mu.npy',mu)\n",
    "# np.save('std_dev.npy',std_dev)\n",
    "# X_train_mfcc -= mu\n",
    "# X_test_mfcc -= mu\n",
    "# X_train_mfcc /= std_dev\n",
    "# X_test_mfcc /= std_dev\n",
    "# print(X_train_mfcc.shape)\n",
    "# X_train_mfcc = X_train_mfcc[...,np.newaxis]\n",
    "# X_test_mfcc = X_test_mfcc[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
