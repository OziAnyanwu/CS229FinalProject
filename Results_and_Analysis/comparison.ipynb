{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_mfccs = np.load('../util/X_mfccs/X_mfccs_4000.npy')\n",
    "X_transcripts = np.load('../util/X_transcripts/X_transcripts_4000.npy')\n",
    "Y = np.load('../util/labels/labels_4000.npy')\n",
    "X_train_mfcc, X_test_mfcc, X_train_transcript, X_test_transcript, y_train, y_test = train_test_split(X_mfccs,X_transcripts,Y,test_size=0.1, random_state = 42)\n",
    "\n",
    "# load models\n",
    "rf = load('rf.joblib')\n",
    "dl = tf.keras.models.load_model(\"tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using rf\n",
    "rf_predictions = rf.predict_proba(X_test_mfcc.reshape(X_test_mfcc.shape[0], -1))[:,1]\n",
    "\n",
    "# predict using dl\n",
    "dl_predictions = dl.predict([X_test_mfcc,X_test_transcript]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc curves\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, dl_predictions)\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf_predictions)\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='DL (area = {:.3f})'.format(auc_keras))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print AP scores\n",
    "\n",
    "average_precision_rf = average_precision_score(y_test, rf_predictions)\n",
    "\n",
    "print('Average random forest precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision_rf))\n",
    "\n",
    "average_precision_dl = average_precision_score(y_test, dl_predictions)\n",
    "\n",
    "print('Average deep learning precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision matrix for random forest\n",
    "\n",
    "disp = plot_confusion_matrix(rf, X_test_mfcc.reshape(X_test_mfcc.shape[0], -1), y_test,\n",
    "                                 cmap=plt.cm.Blues)\n",
    "disp.ax_.set_title(\"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to define our own function to plot the decision matrix for deep learning, \n",
    "# since it's not a scikit-learn classifier\n",
    "\n",
    "def plot_confusion_matrix_custom(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix for deep learning\n",
    "\n",
    "cm = confusion_matrix(y_test, np.round(dl_predictions))\n",
    "plot_confusion_matrix_custom(cm, [\"0\", \"1\"], \"Deep Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions to print accuracies\n",
    "\n",
    "def find_counts(predictions, truth):\n",
    "    TP = 0; TN = 0; FP = 0; FN = 0\n",
    "    for i in range(predictions.shape[0]):\n",
    "        if predictions[i] < 0.5 and truth[i] == 0:\n",
    "            TN += 1\n",
    "        elif predictions[i] >= 0.5 and truth[i] == 1:\n",
    "            TP += 1\n",
    "        elif predictions[i] < 0.5 and truth[i] == 1:\n",
    "            FN += 1\n",
    "        elif predictions[i] >= 0.5 and truth[i] == 0:\n",
    "            FP += 1\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def find_accuracies(TP, TN, FP, FN):\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    a_0 = TP / (TP + FN)\n",
    "    a_1 = TN / (TN + FP)\n",
    "    balanced_accuracy = 1/2 * (a_0 + a_1)\n",
    "    return accuracy, balanced_accuracy, a_0, a_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracies \n",
    "\n",
    "TP, TN, FP, FN = find_counts(rf_predictions, y_test)\n",
    "print(find_accuracies(TP, TN, FP, FN))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229proj",
   "language": "python",
   "name": "cs229proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
