{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "\n",
    "def find_counts(predictions, truth):\n",
    "    TP = 0; TN = 0; FP = 0; FN = 0\n",
    "    for i in range(predictions.shape[0]):\n",
    "        if predictions[i] < 0.5 and truth[i] == 0:\n",
    "            TN += 1\n",
    "        elif predictions[i] >= 0.5 and truth[i] == 1:\n",
    "            TP += 1\n",
    "        elif predictions[i] < 0.5 and truth[i] == 1:\n",
    "            FN += 1\n",
    "        elif predictions[i] >= 0.5 and truth[i] == 0:\n",
    "            FP += 1\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def find_accuracies(TP, TN, FP, FN):\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    a_0 = TP / (TP + FN)\n",
    "    a_1 = TN / (TN + FP)\n",
    "    balanced_accuracy = 1/2 * (a_0 + a_1)\n",
    "    return accuracy, balanced_accuracy, a_0, a_1\n",
    "\n",
    "def main():\n",
    "    X = np.load(\"../util/X_mfccs/X_mfccs_4000.npy\")\n",
    "    X = np.reshape(X, (X.shape[0], -1))\n",
    "    y = np.load(\"../util/labels/labels_4000.npy\")\n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "    theta_0 = np.zeros((x_train.shape[1],))\n",
    "    clf = LogisticRegression(theta_0 = theta_0, verbose = True)\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    predictions = clf.predict(x_valid)\n",
    "    TP, TN, FP, FN = find_counts(predictions, y_valid)\n",
    "    print(find_accuracies(TP, TN, FP, FN))\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    \"\"\"Logistic regression with Newton's Method as the solver.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = LogisticRegression()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "    def __init__(self, step_size=0.01, max_iter=10, eps=1e-5,\n",
    "                 theta_0=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            step_size: Step size for iterative solvers only.\n",
    "            max_iter: Maximum number of iterations for the solver.\n",
    "            eps: Threshold for determining convergence.\n",
    "            theta_0: Initial guess for theta. If None, use the zero vector.\n",
    "            verbose: Print loss values during training.\n",
    "        \"\"\"\n",
    "        self.theta = theta_0\n",
    "        self.step_size = step_size\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Run Newton's Method to minimize J(theta) for logistic regression.\n",
    "\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (n_examples, dim).\n",
    "            y: Training example labels. Shape (n_examples,).\n",
    "        \"\"\"\n",
    "        for _ in range(self.max_iter):\n",
    "            g = gradient(x, y, self.theta)\n",
    "            print(\"calculated gradient\")\n",
    "            h = hessian(x, y, self.theta)\n",
    "            print(\"calculated hessian\")\n",
    "            old_theta = self.theta\n",
    "            self.theta = old_theta - self.step_size * np.dot(np.linalg.inv(h), g)\n",
    "            if self.verbose:\n",
    "                print(loss(x, y, self.theta))\n",
    "            if np.linalg.norm(self.theta - old_theta, ord = 1) < self.eps:\n",
    "                break\n",
    "            \n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Return predicted probabilities given new inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (n_examples, dim).\n",
    "\n",
    "        Returns:\n",
    "            Outputs of shape (n_examples,).\n",
    "        \"\"\"\n",
    "        return sigmoid(np.dot(x, self.theta))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def gradient(x, y, theta):\n",
    "    XT_theta = np.dot(x, theta)\n",
    "    sigmoid_XT_theta = sigmoid(XT_theta)\n",
    "    S = y - sigmoid_XT_theta\n",
    "    return (-1/x.shape[0]) * np.dot(x.T, S)\n",
    "\n",
    "def loss(x, y, theta):\n",
    "    loss = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        if (y[i]) == 1:\n",
    "            loss += y[i]*np.log(sigmoid(np.dot(theta.T, x[i, :])))\n",
    "        if (y[i]) == 0:\n",
    "            loss += (1-y[i])*np.log(1-sigmoid(np.dot(theta.T, x[i, :])))\n",
    "    return (-1/y.shape[0] * loss)\n",
    "\n",
    "def hessian(x, y, theta):\n",
    "    XT_theta = np.dot(x, theta)\n",
    "    sigmoid_XT_theta = sigmoid(XT_theta)\n",
    "    diag = sigmoid_XT_theta * (1 - sigmoid_XT_theta)\n",
    "    G = np.diag(diag)\n",
    "    return (1/x.shape[0]) * np.dot(np.dot(x.T, G), x)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
